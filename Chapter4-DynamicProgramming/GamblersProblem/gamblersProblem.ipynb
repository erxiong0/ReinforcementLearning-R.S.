{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gambler's Problem  \n",
    "  \n",
    "If the coins comes up heads, he wins as many dollars as he has staked on that flip.\n",
    "  \n",
    "- <b>Terminations</b>\n",
    "    - Game ends when gambler wins by reaching goal of \\$100  \n",
    "    - loss by running out of money  \n",
    "- <b>States</b>: gambler's capital, $s\\in \\{1, 2, \\cdots, 99\\}$  \n",
    "  \n",
    "- <b>Actions</b>: stakes, $a\\in \\{0, 1, \\cdots, {\\rm min}(s, 100-s)\\}$  \n",
    "  \n",
    "- <b>Rewards</b>  \n",
    "    - 0 on all transitions \n",
    "    - reaches \\$100, $+1$  \n",
    "  \n",
    "  \n",
    "- <b>state-value function</b>  \n",
    "    - probability of winning from each state  \n",
    "  \n",
    "- <b>A policy</b>  \n",
    "    - The optimal policy maximizes the probability of reaching the goal.  \n",
    "  \n",
    "Let $p_h$ denote the probability of the coin coming up heads. Shows the change in the value function over successive sweeps of value iteration, and the final policy found, for the case of $p_h=0.4$. This policy is optimal, but not unique. In fact, there is a whole family of optimal policies, all corresponding to ties for the argmax action selection with respect to the optimal value function. Can you guess what the entire family looks like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. state value, a matrix with shape (state, action)\n",
    "# 2. p(s',r|s, a) is equals to 0.4\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "policy = np.zeros(100, 100)\n",
    "state = np.arange(1, 100)\n",
    "stateValue = np.arange(1, 100)\n",
    "\n",
    "DISCOUNT = 0.9\n",
    "\n",
    "def eposidic_game():\n",
    "    for s in state:\n",
    "        for a in range(1, s + 1):\n",
    "            next_win = min(s + 2 * a, 100)   # win the flip\n",
    "            next_loss = max(0, s - 2 * a)\n",
    "\n",
    "            if next_win == 100:\n",
    "                return 1\n",
    "            \n",
    "            elif next_loss == 0:\n",
    "                return 0\n",
    "            \n",
    "            policy[s, a] =  0.4 * DISCOUNT * policy[next_win,:] + 0.4 * DISCOUNT * policy[next_loss,:]\n",
    "        \n",
    "        stateValue[s] = np.argmax(policy[s,:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
